
# architecture
architecture = "models.unimanual2bimanual.kit_mocap"

# log and weights will be stored at log_root/experiment_name and weight_root/experiment_name respectively
# experiment_name != result_name so I can store the results given various settings
log_root		= "~/Forecasting-Bimanual-Object-Manipulation-Sequences-From-Unimanual-Observations/logs/unimanual2bimanual"
weight_root 	= "~/Forecasting-Bimanual-Object-Manipulation-Sequences-From-Unimanual-Observations/weights/unimanual2bimanual"
experiment_name	= "ours/kit_mocap/joint"
log_name 		= None

# dataset will be loaded from data_root/data_folder
data_loader			= "dataloaders.unimanual2bimanual.kit_mocap"
data_root			= "~/Forecasting-Bimanual-Object-Manipulation-Sequences-From-Unimanual-Observations/datasets/kit_mocap"
data_name			= "data"
# in case i want to manually provide a path to the cached data
cached_data_path	= None

# dataset settings
main_actions 			= ["Cut", "Mix", "Peel", "Pour", "RollOut", "Scoop", "Stir", "Transfer", "Wipe"]
main_actions_scale		= []
sample_ratio			= [5.35, 8.91, 8.91, 1.3, 9.72, 1, 1.7, 1.49, 1.99]
fine_actions			= ["Approach", "Move", "Hold", "Place", "Retreat", "Idle"]
resolution				= 0.2
inp_length				= 10 
out_length				= 20 
time_step_size  		= 0.10
xyz_scale				= 0.1
kin_scale				= 1
add_distractors			= 1
num_extra_distractors	= -1
object_padded_length 	= 8
pose_padded_length		= 0
train_samples			= [0,1]
val_samples				= [2]
truncate_sequence		= None
mocap_var_limit			= 1.0
p						= None

# optimization
num_workers			= 16
batch_size			= 128
seed			 	= 1337
lr					= 1e-3
tr_step				= 2
va_step				= 2
teacher_force_ratio	= 0.6
loss_names			= ["inp_obj_xyz", "inp_handled_obj", "free_net_inp_xyz", "free_net_inp_finger", "grab_net_inp_xyz", "grab_net_inp_finger","out_obj_xyz","lhand_out_handled_obj","rhand_out_handled_obj","free_net_out_xyz","free_net_out_finger","grab_net_out_xyz","grab_net_out_finger","grab_net_agg_xyz","grab_net_agg_finger"]
loss_functions		= ["self_mse", "self_cross_entropy","self_mse","self_mse","self_mse","self_mse","self_mse","self_cross_entropy","self_cross_entropy","self_mse","self_mse","self_mse","self_mse","self_mse","self_mse"]
loss_weights		= ["[1e-1]*1000","[1]*1000","[1e-1]*1000","[1e-1]*1000","[1e-1]*1000","[1e-1]*1000","[1e-2]*1000","[1e-1]*1000","[1e-1]*1000","[1e-2]*1000","[1e-2]*1000","[1e-2]*1000","[1e-2]*1000","[0]*1000","[0]*1000"]
task_names			= ["encoder","decoder"]
task_components		= [["inp_obj_xyz", "inp_handled_obj", "free_net_inp_xyz", "free_net_inp_finger", "grab_net_inp_xyz", "grab_net_inp_finger"],["out_obj_xyz","lhand_out_handled_obj","rhand_out_handled_obj","free_net_out_xyz","free_net_out_finger","grab_net_out_xyz","grab_net_out_finger","grab_net_agg_xyz","grab_net_agg_finger"]]
freeze				= None
freeze_layer_names	= None
freeze_epochs		= None
reset_loss			= None

# # # # # # # # # # #
# encoder backbone  #
# # # # # # # # # # #

# object label and position embedder
object_label_embedder_units		= [22, 256]
position_embedder_units			= 256

# individual encoders
inp_object_encoder_units		= [32, 256]
inp_object_encoder_activations	= ["none"]
inp_object_encoder_data_type	= ["masked_obj_xyz","obj_ohs"]
inp_human_encoder_units			= [197, 256]
inp_human_encoder_activations	= ["none"]
inp_human_encoder_data_type		= ["masked_xyz","masked_finger"]

# temporal encoder
temporal_encoder_type			= "GraphGRU"
transformer_encoder_units		= []
transformer_encoder_data_type	= ""
graph_encoder_units				= [[],[512,256,1,False,512,256]]
graph_encoder_activations		= [[],["none"]]
graph_encoder_type				= "graph_recurrent"

# # # # # # # # # # #
# decoder backbone  #
# # # # # # # # # # #

# individual encoders
out_object_encoder_units		= [32, 256]
out_object_encoder_activations	= ["none"]
out_object_encoder_data_type	= ["obj_xyz","obj_ohs"]
out_human_encoder_units			= [159, 256]
out_human_encoder_activations	= ["none"]
out_human_encoder_data_type		= ["xyz"]

# temporal decoder
temporal_decoder_type			= "GraphGRU"
transformer_decoder_units		= []
transformer_decoder_data_type	= ""
transformer_decoder_memory_type = ""
graph_decoder_units				= [[],[512,256,1,False,512,256]]
graph_decoder_activations		= [[],["none"]]
graph_decoder_type				= "graph_recurrent"

# # # # # # # # # # # # # # #
# ranked prediction module  #
# # # # # # # # # # # # # # #

# object decoder
omm_object_decoder_units		= [256, 12]
omm_object_decoder_activations  = ["none"]
omm_object_decoder_data_type	= [""]

# object classifier
omm_object_classifier_units 		= [276, 256, 1]
omm_object_classifier_activations 	= ["relu", "none"]
omm_object_classifier_type			= "mlp"
omm_object_classifier_data_type 	= ["obj_ohs"]

# # # # # # # # # # # # # # #
# ensemble selector module  #
# # # # # # # # # # # # # # #

# free net
free_net_human_decoder_units		= [256, 197]
free_net_human_decoder_activations	= ["none"]
free_net_human_decoder_type			= "mlp"
free_net_human_decoder_output_type	= ["xyz","finger"]

# grab net encoders
grab_net_object_encoder_units		= []
grab_net_object_encoder_activations	= []
grab_net_object_encoder_data_type	= ["handled_obj_xyz","handled_obj_ohs"]
grab_net_human_encoder_units		= []
grab_net_human_encoder_activations	= []
grab_net_human_encoder_data_type	= ["masked_xyz"]

# grab_net human decoder
grab_net_human_decoder_units		= [191, 512, 159]
grab_net_human_decoder_activations	= ["relu", "none"]
grab_net_human_decoder_type			= "mlp"
grab_net_human_decoder_prior		= ""
grab_net_human_decoder_posterior	= ""
grab_net_output_type				= "full_body"
grab_net_agg_mode					= "train"

# grab_net finger decoder
grab_net_num_finger_decoders		= 2
grab_net_finger_decoder_units		= [32, 256, 19]
grab_net_finger_decoder_activations	= ["relu","none"]
grab_net_finger_decoder_type		= "mlp"
grab_net_finger_decoder_data_type	= ""
grab_net_finger_decoder_posterior	= ""

# # # # # # # # # # # # # # # #
# other architecture settings #
# # # # # # # # # # # # # # # #

forecast = 1
additional_operations 	= 0
residual_features	  	= 0
predict_body_velocity 	= 1
predict_object_velocity = 1

# checkpointing
custom_loader			= 0
restore_from_checkpoint	= 0
checkpoint_task_names 	= ["encoder","decoder"]
checkpoint_epoch_names 	= [-1,-1]
checkpoint_layer_names	= [["reconstruction_module"],["forecasting_module"]]
strict = 0
remove = ["prior","posterior","distribution"]

# load pre-trained (only for training)
load_pretrained_weight		= 0
pretrained_weight_root 		= "~/Forecasting-Bimanual-Object-Manipulation-Sequences-From-Unimanual-Observations/weights/unimanual2bimanual"
pretrained_experiment_name	= "2023_04_05/inp=10_out=20_graph_velocity=1_loss=1e-1_linear_res=0.2_b=128_peel_pour"
pretrained_task_names		= ["decoder"]
pretrained_epoch_names		= [-1]
pretrained_layer_names		= [["out_object_label_embedder","out_human_encoder", "out_object_encoder", "temporal_encoder", "temporal_decoder", "out_human_decoder", "out_object_decoder","out_object_classifier"]]
pretrained_strict			= 0
pretrained_load_optimizer	= 0

# json results will be stored in result_root/result_name
# experiment_name != result_name so I can store the results given various settings
test_dtype  	= "val"
result_root		= "~/Forecasting-Bimanual-Object-Manipulation-Sequences-From-Unimanual-Observations/results/unimanual2bimanual" 
result_name		= "kit_mocap/joint"